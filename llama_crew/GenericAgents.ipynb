{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b523e0a",
   "metadata": {},
   "source": [
    "# Lesson 4: Building Agents\n",
    "\n",
    "https://github.com/run-llama/llama_index/blob/767de070b231fb328b6c0640c2e002c9c7af0a83/docs/docs/examples/agent/custom_agent.ipynb#L12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a323703",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9625ab2-71b6-4fd0-904e-42df80d3215f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from helper import get_openai_api_key\n",
    "OPENAI_API_KEY = get_openai_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3221a474-5817-4db2-af46-e029042a75a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20adaa26",
   "metadata": {},
   "source": [
    "## 1. Setup an agent with tools loaded from yaml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f26bbac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import importlib\n",
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "class Tool:\n",
    "    def __init__(self, config):\n",
    "        self.name = config['name']\n",
    "        self.module = config['module']\n",
    "        self.function = config['function']\n",
    "        self.instance = self.load_tool()\n",
    "\n",
    "    def load_tool(self):\n",
    "        module = importlib.import_module(self.module)\n",
    "        tool_func = getattr(module, self.function)\n",
    "        return tool_func\n",
    "\n",
    "def load_tools_config(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        tools_config = yaml.safe_load(file)\n",
    "    return tools_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7bef8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools_config = load_tools_config('tools.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41598b5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tools': [{'name': 'add', 'module': 'tools.sample_tools', 'function': 'add'},\n",
       "  {'name': 'subtract', 'module': 'tools.sample_tools', 'function': 'subtract'},\n",
       "  {'name': 'multiply', 'module': 'tools.sample_tools', 'function': 'multiply'},\n",
       "  {'name': 'divide', 'module': 'tools.sample_tools', 'function': 'divide'},\n",
       "  {'name': 'search',\n",
       "   'module': 'tools.search_tools',\n",
       "   'function': 'search_ddg'}]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1badd205",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tools = [Tool(config) for config in tools_config[\"tools\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bff58c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80bac8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_tools = [ FunctionTool.from_defaults(fn=fn.instance) for fn in all_tools]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f2c6a9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(initial_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a124a438-5609-402e-8642-69d1088cb9ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    initial_tools, \n",
    "    llm=llm, \n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17409d4c-05a9-4bf4-b74f-75135fa3cb6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# response = agent.query(\n",
    "#     \"Think step by step, How much is 2 + 2 * 3\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ace340b1-761f-4058-be41-68cf131541e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# response = agent.query(\"How much is 3.14159 * 2.334\")\n",
    "# print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b58b160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = agent.query(\"How much is 10 / 3 * 3\")\n",
    "# print(str(response)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b99a186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = agent.query(\"Think step by step, how much is 10 / 3 * 3\")\n",
    "# print(str(response)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b45bc177",
   "metadata": {},
   "outputs": [],
   "source": [
    "agents_config = yaml.safe_load(open('agents.yaml', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "231c71c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agents': [{'name': 'mathematician',\n",
       "   'role': 'Answer mathematics questions',\n",
       "   'prompt': 'You are an expert in mathematics, you will answer questions related to maths.',\n",
       "   'tools': ['add', 'subtract', 'multiply', 'divide'],\n",
       "   'verbose': True},\n",
       "  {'name': 'online_research',\n",
       "   'role': 'You know how to search in duckduckgo',\n",
       "   'prompt': 'You are an expert in online search',\n",
       "   'tools': ['search'],\n",
       "   'verbose': True},\n",
       "  {'name': 'oracle',\n",
       "   'role': \"Answer questions about people's life and death\",\n",
       "   'prompt': \"You are the Oracle and you make up stories about people's life and death.\",\n",
       "   'verbose': True}]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agents_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79285c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce79fbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.bridge.pydantic import PrivateAttr\n",
    "from llama_index.core.tools import BaseTool, QueryEngineTool\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "abda362c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RouterQueryEngine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e22b4b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import (\n",
    "    CustomSimpleAgentWorker,\n",
    "    Task,\n",
    "    AgentChatResponse,\n",
    ")\n",
    "from typing import Dict, Any, List, Tuple, Optional\n",
    "from llama_index.core.query_engine import RouterQueryEngine\n",
    "from llama_index.core import ChatPromptTemplate, PromptTemplate\n",
    "from llama_index.core.selectors import PydanticSingleSelector\n",
    "from llama_index.core.bridge.pydantic import Field, BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "582eb364",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_index.core.llms import ChatMessage, MessageRole\n",
    "\n",
    "DEFAULT_PROMPT_STR = \"\"\"\n",
    "Given previous question/response pairs, please determine if an error has occurred in the response, and suggest \\\n",
    "a modified question that will not trigger the error.\n",
    "\n",
    "Examples of modified questions:\n",
    "- The question itself is modified to elicit a non-erroneous response\n",
    "- The question is augmented with context that will help the downstream system better answer the question.\n",
    "- The question is augmented with examples of negative responses, or other negative questions.\n",
    "\n",
    "An error means that either an exception has triggered, or the response is completely irrelevant to the question.\n",
    "\n",
    "Please return the evaluation of the response in the following JSON format.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_chat_prompt_template(\n",
    "    system_prompt: str, current_reasoning: Tuple[str, str]\n",
    ") -> ChatPromptTemplate:\n",
    "    system_msg = ChatMessage(role=MessageRole.SYSTEM, content=system_prompt)\n",
    "    messages = [system_msg]\n",
    "    for raw_msg in current_reasoning:\n",
    "        if raw_msg[0] == \"user\":\n",
    "            messages.append(\n",
    "                ChatMessage(role=MessageRole.USER, content=raw_msg[1])\n",
    "            )\n",
    "        else:\n",
    "            messages.append(\n",
    "                ChatMessage(role=MessageRole.ASSISTANT, content=raw_msg[1])\n",
    "            )\n",
    "    return ChatPromptTemplate(message_templates=messages)\n",
    "\n",
    "class ResponseEval(BaseModel):\n",
    "    \"\"\"Evaluation of whether the response has an error.\"\"\"\n",
    "\n",
    "    has_error: bool = Field(\n",
    "        ..., description=\"Whether the response has an error.\"\n",
    "    )\n",
    "    new_question: str = Field(..., description=\"The suggested new question.\")\n",
    "    explanation: str = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"The explanation for the error as well as for the new question.\"\n",
    "            \"Can include the direct stack trace as well.\"\n",
    "        ),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3343c64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAgentWorker(CustomSimpleAgentWorker):\n",
    "    \"\"\"Agent worker that adds a retry layer on top of a router.\n",
    "\n",
    "    Continues iterating until there's no errors / task is done.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    prompt_str: str = Field(default=DEFAULT_PROMPT_STR)\n",
    "    role_prompt: str = Field(default=\"You are a AI assistant.\")\n",
    "    max_iterations: int = Field(default=3)\n",
    "    __fields_set__: set =  {\"prompt_str\", \"max_iterations\",\"role_prompt\"}\n",
    "    _router_query_engine: RouterQueryEngine = PrivateAttr()\n",
    "    \n",
    "\n",
    "    def __init__(self, **kwargs: Any) -> None:\n",
    "        \"\"\"Initialize agent worker.\"\"\"\n",
    "        self.tools = []\n",
    "        self.role_prompt = kwargs.get(\"role_prompt\", \"\") + DEFAULT_PROMPT_STR\n",
    "        self._router_query_engine = RouterQueryEngine(\n",
    "            selector=PydanticSingleSelector.from_defaults(),\n",
    "            query_engine_tools=self.tools,\n",
    "            verbose=kwargs.get(\"verbose\", False),\n",
    "        )\n",
    "        super().__init__(\n",
    "            tools=[],\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def _initialize_state(self, task: Task, **kwargs: Any) -> Dict[str, Any]:\n",
    "        \"\"\"Initialize state.\"\"\"\n",
    "        return {\"count\": 0, \"current_reasoning\": []}\n",
    "\n",
    "    def _run_step(\n",
    "        self, state: Dict[str, Any], task: Task, input: Optional[str] = None\n",
    "    ) -> Tuple[AgentChatResponse, bool]:\n",
    "        \"\"\"Run step.\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (agent_response, is_done)\n",
    "\n",
    "        \"\"\"\n",
    "        if \"new_input\" not in state:\n",
    "            new_input = task.input\n",
    "        else:\n",
    "            new_input = state[\"new_input\"]\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"> Querying engine: {new_input}\")\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"> Prompt: {self.role_prompt}\")\n",
    "        response = self.llm.complete(self.role_prompt + new_input)\n",
    "\n",
    "            \n",
    "        # append to current reasoning\n",
    "        state[\"current_reasoning\"].extend(\n",
    "            [(\"user\", new_input), (\"assistant\", str(response))]\n",
    "        )\n",
    "        is_done = True\n",
    "        \n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"> Question: {new_input}\")\n",
    "            print(f\"> Response: {response}\")\n",
    "            # print(f\"> Response eval: {response_eval.dict()}\")\n",
    "\n",
    "        # return response\n",
    "        return AgentChatResponse(response=str(response)), is_done\n",
    "\n",
    "    def _finalize_task(self, state: Dict[str, Any], **kwargs) -> None:\n",
    "        \"\"\"Finalize task.\"\"\"\n",
    "        # nothing to finalize here\n",
    "        # this is usually if you want to modify any sort of\n",
    "        # internal state beyond what is set in `_initialize_state`\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5bac609d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for agent_config in agents_config[\"agents\"]:\n",
    "    initial_tools = [ FunctionTool.from_defaults(fn=fn.instance) for fn in all_tools if fn.name in agent_config.get(\"tools\",[])]\n",
    "    if len(initial_tools) == 0:\n",
    "        agent_worker = SimpleAgentWorker(llm=llm, role_prompt=agent_config[\"prompt\"], can_delegate=agent_config.get(\"can_delegate\",False), verbose=True)\n",
    "    else:\n",
    "        agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "            initial_tools, \n",
    "            llm=llm, \n",
    "            verbose=True\n",
    "        )\n",
    "    agent = AgentRunner(agent_worker)\n",
    "    agents[agent_config[\"name\"]] = agent\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af243cdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['mathematician', 'online_research', 'oracle'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agents.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d46c3d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agents[\"oracle\"].query(\" When will I get married?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "112b6eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agents[\"oracle\"].state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f7f29d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agents[\"mathematician\"].query(\"How much is 10 / 3 * 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d214b838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agents[\"mathematician\"].state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "42854ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agents['ceo_expert'].query(\"What is the best way to increase revenue?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c7f55ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Orchestrator:\n",
    "    system_prompt = \"You are the orchestrator. You can ask any of the agents: [{agents_list}] and forward the response to the user Given the follow question from the user:\\n\"\n",
    "    def __init__(self, llm, agents, **kwargs):\n",
    "        self.llm = llm\n",
    "        self.agents = agents\n",
    "        self.agents_config = kwargs.get(\"agents_config\", {})\n",
    "        self.verbose = kwargs.get(\"verbose\", False)\n",
    "        \n",
    "    def query(self, query):\n",
    "        prompt = self.system_prompt.format(agents_list=[agent[\"name\"]+ \":\" + agent[\"role\"] for agent in self.agents_config[\"agents\"]])\n",
    "        step = self.llm.complete( prompt + query +  \"\\n\\nRespond with the name of the agent to call and the query to forward to the agent in the following format: 'agent_name: query'\")\n",
    "        agent_name, agent_query = str(step).split(\":\")\n",
    "        if self.verbose:\n",
    "            print(f\"Forwarding the query to the agent {agent_name} with the query: {agent_query}\")\n",
    "        response = self.query_agent(agent_name.strip(), agent_query.strip())\n",
    "        if self.verbose:\n",
    "            print(f\"Agent {agent_name} responded with: {response}\")\n",
    "        eval_response = self.eval_response(query, agent_name.strip(), agent_query.strip(), response)\n",
    "        if self.verbose:\n",
    "            print(f\"Response evaluation: {eval_response}\")\n",
    "        return response, eval_response\n",
    "        \n",
    "    \n",
    "    def eval_response(self, task, agent_name, query, response):\n",
    "        system_prompt = f\"Given the follow question from the user:\\n{task}\\n\\nThe response from the agent {agent_name} to the query {query} is:\\n{response}\\n\\nPlease evaluate the response in the following format: 'has_error: new_question: explanation'\"\n",
    "        return self.llm.complete(system_prompt)\n",
    "\n",
    "    def query_agent(self, agent_name, query):\n",
    "        return self.agents[agent_name].query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3475fcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "director = Orchestrator(llm, agents, agents_config=agents_config, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "210c82d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# director.query(\"Ask the oracle When will I get married?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b08d9907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# director.query(\"How much is 10 / 3 * 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d9efc4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forwarding the query to the agent online_research with the query:  Search for the dollar price of ethereum on May 6th, 2024.\n",
      "Added user message to memory: Search for the dollar price of ethereum on May 6th, 2024.\n",
      "=== Calling Function ===\n",
      "Calling function: search_ddg with args: {\"query\": \"Ethereum price on May 6th, 2024 in USD\"}\n",
      "=== Function Output ===\n",
      "[{'title': 'Ethereum USD (ETH-USD) Stock Historical Prices & Data - Yahoo Finance', 'href': 'https://finance.yahoo.com/quote/ETH-USD/history/', 'body': \"Discover historical prices for ETH-USD stock on Yahoo Finance. View daily, weekly or monthly format back to when Ethereum USD stock was issued. News. Today's news; US; ... May 6, 2024: 3,137.51: ...\"}, {'title': 'Ethereum USD (ETH-USD) Price History & Historical Data - Yahoo Finance', 'href': 'https://ca.finance.yahoo.com/quote/ETH-USD/history/', 'body': '2,298.89. 7,277,068,110. *Close price adjusted for splits. **Adjusted close price adjusted for splits and dividend and/or capital gain distributions. Loading more data... Discover historical prices of Ethereum USD (ETH-USD) on Yahoo Finance. View daily, weekly or monthly formats.'}, {'title': 'Ethereum Price | ETH Price and Live Chart - CoinDesk', 'href': 'https://www.coindesk.com/price/ethereum/', 'body': 'The Ethereum price is $2,985.06, a change of 0.98% over the past 24 hours as of 1:57 p.m. ... May 9, 2024 at 10:10 p.m. UTC. May 9, 2024 ... benchmarking billions of dollars in registered ...'}]\n",
      "=== LLM Response ===\n",
      "The price of Ethereum on May 6th, 2024, was $3,137.51 USD.\n",
      "Agent online_research responded with: assistant: The price of Ethereum on May 6th, 2024, was $3,137.51 USD.\n",
      "Response evaluation: has_error: No\n",
      "new_question: N/A\n",
      "explanation: The response provided by the agent is accurate and directly answers the user's question about the dollar price of Ethereum on May 6th, 2024.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Response(response='assistant: The price of Ethereum on May 6th, 2024, was $3,137.51 USD.', source_nodes=[], metadata=None),\n",
       " CompletionResponse(text=\"has_error: No\\nnew_question: N/A\\nexplanation: The response provided by the agent is accurate and directly answers the user's question about the dollar price of Ethereum on May 6th, 2024.\", additional_kwargs={}, raw={'id': 'chatcmpl-9Pz5a7j27575drasiQvGWFQgkTrE4', 'choices': [Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"has_error: No\\nnew_question: N/A\\nexplanation: The response provided by the agent is accurate and directly answers the user's question about the dollar price of Ethereum on May 6th, 2024.\", role='assistant', function_call=None, tool_calls=None))], 'created': 1715980054, 'model': 'gpt-3.5-turbo-0125', 'object': 'chat.completion', 'system_fingerprint': None, 'usage': CompletionUsage(completion_tokens=45, prompt_tokens=105, total_tokens=150)}, logprobs=None, delta=None))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "director.query(\"What was the dollar price on ethereum back in May 6th 2024?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6027d835",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
